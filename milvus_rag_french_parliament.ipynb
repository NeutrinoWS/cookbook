{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG application with Milvus Lite, Mistral and Llama-index\n",
    "\n",
    "In this notebook, we are showing how you can build a Retrieval Augmented Generation (RAG) application to interact with data from the French Parliament. It uses Ollama with Mistral for LLM operations, Llama-index for orchestration, and Milvus for vector storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the different dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymilvus ollama llama-index-llms-ollama llama-index-vector-stores-milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-huggingface sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Embedding model\n",
    "\n",
    "Given that the text is written in French, I am using an embedding model that is specially trained on French Language, feel free to use a different one you can find on HuggingFace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embeddings = HuggingFaceEmbedding(model_name=\"dangvantuan/sentence-camembert-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare out data to be stored in Milvus\n",
    "\n",
    "This code makes it possible to process text embeddings using Sentence Camembert Large & Mistral-7B and store those in Milvus.\n",
    "\n",
    "**!!Make sure to have Ollama running on your laptop!!**\n",
    "\n",
    "* Initialises Mistral-7B model using Ollama\n",
    "* Service Context: Configures a service context with Mistral and the embedding model defined above\n",
    "* Vector Store: Sets up a collection in Milvus to store text embeddings, specifying the database file, collection name, vector dimensions\n",
    "* Storage Context: Configures a storage context with the Milvus vector store\n",
    "\n",
    "This makes it possible to have efficient storage and retrieval of vector embeddings for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "from llama_index.core import StorageContext, ServiceContext\n",
    "\n",
    "llm = Ollama(model=\"mistral\", request_timeout=120.0)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embeddings, chunk_size=350)\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"milvus_mistral_rag.db\",\n",
    "    collection_name=\"mistral_french_parliament\",\n",
    "    dim=1024,  # the value changes with embedding model\n",
    "    overwrite=True  # drop table if exist and then create\n",
    "    \n",
    "    )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=['data/french_parliament_discussion.xml']).load_data()\n",
    "vector_index = VectorStoreIndex.from_documents(docs, storage_context=storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool, ToolMetadata\n",
    "\n",
    "milvus_tool_openai = RetrieverTool(\n",
    "    retriever=vector_index.as_retriever(similarity_top_k=3),  # retrieve top_k results\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"CustomRetriever\",\n",
    "        description='Retrieve relevant information from provided documents.'\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, ask questions to our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The French parliament discussed a proposed law and the ongoing health crisis management. They expressed their past efforts in debating and suggesting amendments, but faced obstacles due to partisan politics. They also welcomed the involvement of commission members Philippe Gosselin and Sacha Houlié in the discussions. Additionally, they noted the progress made in dealing with the health crisis issues more dispassionately, and looked forward to finding compromises through legislative work. Amendments related to this topic were identified for further examination.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the French parliament talk about the last time?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In recent parliamentary discussions, there have been debates surrounding protective measures for the population regarding travel to and from French territory. The necessity of widespread testing has been emphasized due to the increase in contaminations. To ensure accessibility to these tests regardless of income, it is proposed that they be made free and available as soon as possible.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"Peux tu me dire de quoi à parler le parliement dernièrement? Je veux la réponse en Français\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
