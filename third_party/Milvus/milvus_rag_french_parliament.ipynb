{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG application with Milvus Lite, Mistral and Llama-index\n",
    "\n",
    "In this notebook, we are showing how you can build a Retrieval Augmented Generation (RAG) application to interact with data from the French Parliament. It uses Ollama with Mistral for LLM operations, Llama-index for orchestration, and [Milvus](https://milvus.io/) for vector storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Ollama\n",
    "\n",
    "Make sure to have Ollama installed and Running on your laptop --> https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the different dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pymilvus ollama llama-index-llms-ollama llama-index-vector-stores-milvus llama-index-readers-file llama-index-embeddings-mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Mistral Embedding\n",
    "\n",
    "Make sure to create an [API Key](https://console.mistral.ai/api-keys/) on Mistral's platform and load it as an environment variable.\n",
    "\n",
    "On this tutorial, we are loading the environment variable stored in our `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "model_name = \"mistral-embed\"\n",
    "embed_model = MistralAIEmbedding(model_name=model_name, api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare out data to be stored in Milvus\n",
    "\n",
    "This code makes it possible to process text embeddings using Sentence Camembert Large & Mistral-7B and store those in Milvus.\n",
    "\n",
    "**!!Make sure to have Ollama running on your laptop!!**\n",
    "\n",
    "* Initialises Mistral-7B model using Ollama\n",
    "* Service Context: Configures a service context with Mistral and the embedding model defined above\n",
    "* Vector Store: Sets up a collection in Milvus to store text embeddings, specifying the database file, collection name, vector dimensions\n",
    "* Storage Context: Configures a storage context with the Milvus vector store\n",
    "\n",
    "This makes it possible to have efficient storage and retrieval of vector embeddings for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/3dw9p_ts4b114chqt9m027pc0000gn/T/ipykernel_96753/2601041383.py:8: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=350)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "from llama_index.core import StorageContext, ServiceContext\n",
    "\n",
    "llm = Ollama(model=\"mistral\", request_timeout=120.0)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=350, chunk_overlap=20)\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"milvus_mistral_rag.db\",\n",
    "    collection_name=\"mistral_french_parliament\",\n",
    "    dim=1024, \n",
    "    overwrite=True  # drop table if exist and then create\n",
    "    \n",
    "    )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=['data/french_parliament_discussion.xml']).load_data()\n",
    "vector_index = VectorStoreIndex.from_documents(docs, storage_context=storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool, ToolMetadata\n",
    "\n",
    "milvus_tool_openai = RetrieverTool(\n",
    "    retriever=vector_index.as_retriever(similarity_top_k=3),  # retrieve top_k results\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"CustomRetriever\",\n",
    "        description='Retrieve relevant information from provided documents.'\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, ask questions to our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The French parliament discussed disagreements regarding a text that was presented, with concerns being raised about it not addressing all demands made over the past two years and half. Propositions were made, evaluations requested, and suggestions such as systematically involving the Parliament and having the Scientific Council report its findings were expressed. However, no progress was made on these matters. Additionally, there were discussions about the end of the Assembly's self-censorship by the presidential majority and the decreasing number of laws compared to ordinances. A change in the assembly composition was seen as a hopeful sign for justifiable and strictly-encountered delegation of power to the Government. In a commission of laws, there had already been noted improvements with the current project law discussion.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the French parliament talk about the last time?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you like this tutorial, feel free to reach out on [LinkedIn](https://www.linkedin.com/in/stephen-batifol/), check out [Milvus](https://github.com/milvus-io/milvus) and join our [Discord](https://discord.gg/FG6hMJStWu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
