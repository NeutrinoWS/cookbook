{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Основы RAG с Mistral AI\n",
        "\n",
        "![](../../images/rag.png)\n",
        "\n",
        "Генерация с расширенным поиском (RAG) — это AI-фреймворк, который объединяет возможности LLM и систем информационного поиска. Он полезен для ответов на вопросы или генерации контента с использованием внешних знаний. В RAG есть два основных этапа: 1) поиск: извлечение релевантной информации из базы знаний с помощью текстовых эмбеддингов, хранящихся в векторном хранилище; 2) генерация: вставка релевантной информации в промпт для LLM для генерации информации. В этом руководстве мы рассмотрим очень простой пример RAG с четырьмя реализациями:\n",
        "\n",
        "- RAG с нуля с Mistral\n",
        "- RAG с Mistral и LangChain\n",
        "- RAG с Mistral и LlamaIndex\n",
        "- RAG с Mistral и Haystack\n",
        "\n",
        "## RAG с нуля\n",
        "\n",
        "Этот раздел призван провести вас через процесс создания базового RAG с нуля. У нас две цели: во-первых, предложить пользователям всестороннее понимание внутренней работы RAG и демистифицировать основные механизмы; во-вторых, дать вам необходимые основы для создания RAG с минимальными зависимостями.\n",
        "\n",
        "### Импорт необходимых пакетов\n",
        "Первый шаг — установить необходимые пакеты `mistralai` и `faiss-cpu` и импортировать нужные пакеты:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install faiss-cpu==1.7.4 mistralai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mistralai import Mistral\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "api_key= getpass(\"Введите ваш API ключ\")\n",
        "client = Mistral(api_key=api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Получение данных\n",
        "\n",
        "В этом очень простом примере мы получаем данные из эссе, написанного Полом Грэмом:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
        "text = response.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Мы также можем сохранить эссе в локальный файл:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = open('essay.txt', 'w')\n",
        "f.write(text)\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Разделение документа на чанки\n",
        "\n",
        "В системе RAG крайне важно разделить документ на более мелкие чанки, чтобы было эффективнее идентифицировать и извлекать наиболее релевантную информацию в процессе поиска позже. В этом примере мы просто разделяем текст по символам, объединяем 2048 символов в каждый чанк, и получаем 37 чанков.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_size = 2048\n",
        "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Соображения:\n",
        "- **Размер чанка**: В зависимости от вашего конкретного случая использования может потребоваться настроить или поэкспериментировать с различными размерами чанков и их перекрытием для достижения оптимальной производительности RAG. Например, меньшие чанки могут быть более полезными в процессах поиска, поскольку большие текстовые чанки часто содержат текст-заполнитель, который может затемнить семантическое представление. Таким образом, использование меньших текстовых чанков в процессе поиска может позволить системе RAG идентифицировать и извлекать релевантную информацию более эффективно и точно. Однако стоит учитывать компромиссы, связанные с использованием меньших чанков, такие как увеличение времени обработки и вычислительных ресурсов.\n",
        "\n",
        "- **Как разделять**: Хотя самый простой метод — разделить текст по символам, есть и другие варианты в зависимости от случая использования и структуры документа. Например, чтобы избежать превышения лимитов токенов в вызовах API, может потребоваться разделить текст по токенам. Для поддержания связности чанков может быть полезно разделить текст по предложениям, абзацам или HTML-заголовкам. Если вы работаете с кодом, часто рекомендуется разделять по осмысленным фрагментам кода, например, используя парсер абстрактного синтаксического дерева (AST).\n",
        "\n",
        "### Создание эмбеддингов для каждого текстового чанка\n",
        "\n",
        "Для каждого текстового чанка нам затем нужно создать текстовые эмбеддинги, которые являются числовыми представлениями текста в векторном пространстве. Ожидается, что слова с похожими значениями будут находиться ближе друг к другу или иметь меньшее расстояние в векторном пространстве.\n",
        "\n",
        "Чтобы создать эмбеддинг, используйте API эмбеддингов Mistral и модель эмбеддингов `mistral-embed`. Мы создаем функцию `get_text_embedding` для получения эмбеддинга из одного текстового чанка, а затем используем list comprehension для получения текстовых эмбеддингов для всех текстовых чанков.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_text_embedding(input):\n",
        "    embeddings_batch_response = client.embeddings.create(\n",
        "          model=\"mistral-embed\",\n",
        "          inputs=input\n",
        "      )\n",
        "    return embeddings_batch_response.data[0].embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_embeddings.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Загрузка в векторную базу данных\n",
        "\n",
        "После получения текстовых эмбеддингов обычной практикой является их хранение в векторной базе данных для эффективной обработки и поиска. Существует несколько векторных баз данных на выбор. В нашем простом примере мы используем векторную базу данных с открытым исходным кодом Faiss, которая позволяет эффективный поиск по сходству.\n",
        "\n",
        "С Faiss мы создаем экземпляр класса Index, который определяет структуру индексирования векторной базы данных. Затем мы добавляем текстовые эмбеддинги в эту структуру индексирования.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = text_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(text_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Соображения:\n",
        "- **Векторная база данных**: При выборе векторной базы данных следует учитывать несколько факторов, включая скорость, масштабируемость, облачное управление, расширенную фильтрацию и открытый исходный код против закрытого.\n",
        "\n",
        "### Создание эмбеддингов для вопроса\n",
        "\n",
        "Когда пользователи задают вопрос, нам также необходимо создать эмбеддинги для этого вопроса, используя те же модели эмбеддингов, что и раньше.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"Какие две основные вещи автор делал до колледжа?\"\n",
        "question_embeddings = np.array([get_text_embedding(question)])\n",
        "question_embeddings.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Соображения:\n",
        "- **Гипотетические эмбеддинги документов (HyDE)**: В некоторых случаях вопрос пользователя может быть не самым релевантным запросом для определения релевантного контекста. Вместо этого может быть более эффективным сгенерировать гипотетический ответ или гипотетический документ на основе запроса пользователя и использовать эмбеддинги сгенерированного текста для поиска похожих текстовых чанков.\n",
        "\n",
        "### Извлечение похожих чанков из векторной базы данных\n",
        "\n",
        "Мы можем выполнить поиск в векторной базе данных с помощью `index.search`, которая принимает два аргумента: первый — это вектор эмбеддингов вопроса, а второй — количество похожих векторов для извлечения. Эта функция возвращает расстояния и индексы наиболее похожих векторов на вектор вопроса в векторной базе данных. Затем на основе возвращенных индексов мы можем извлечь фактические релевантные текстовые чанки, соответствующие этим индексам.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "D, I = index.search(question_embeddings, k=2)\n",
        "print(I)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "print(retrieved_chunk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Соображения:\n",
        "- **Методы поиска**: Существует много различных стратегий поиска. В нашем примере мы показываем простой поиск по сходству с эмбеддингами. Иногда, когда доступны метаданные для данных, лучше сначала отфильтровать данные на основе метаданных перед выполнением поиска по сходству. Также существуют другие статистические методы поиска, такие как TF-IDF и BM25, которые используют частоту и распределение терминов в документе для идентификации релевантных текстовых чанков.\n",
        "\n",
        "- **Извлеченный документ**: Всегда ли мы извлекаем отдельный текстовый чанк как есть? Не всегда.\n",
        "    - Иногда мы хотели бы включить больше контекста вокруг фактически извлеченного текстового чанка. Мы называем фактически извлеченный текстовый чанк \"дочерним чанком\", и наша цель — извлечь более крупный \"родительский чанк\", которому принадлежит \"дочерний чанк\".\n",
        "    - Иногда мы также можем захотеть присвоить веса нашим извлеченным документам. Например, взвешенный по времени подход поможет нам извлечь самый последний документ.\n",
        "    - Одна распространенная проблема в процессе поиска — это проблема \"потери в середине\", когда информация в середине длинного контекста теряется. Наши модели пытались смягчить эту проблему. Например, в задаче passkey наши модели продемонстрировали способность находить \"иголку в стоге сена\", извлекая случайно вставленный пароль в длинном промпте, до 32k длины контекста. Однако стоит рассмотреть возможность экспериментов с переупорядочиванием документа, чтобы определить, приведет ли размещение наиболее релевантных чанков в начале и конце к улучшенным результатам.\n",
        "\n",
        "### Объединение контекста и вопроса в промпте и генерация ответа\n",
        "\n",
        "Наконец, мы можем предложить извлеченные текстовые чанки в качестве контекстной информации в промпте. Вот шаблон промпта, где мы можем включить как извлеченный текст, так и вопрос пользователя в промпт.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Контекстная информация приведена ниже.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Учитывая контекстную информацию, а не предварительные знания, ответьте на запрос.\n",
        "Запрос: {question}\n",
        "Ответ:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_mistral(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Соображения:\n",
        "- **Техники промптинга**: Большинство техник промптинга также можно использовать при разработке системы RAG. Например, мы можем использовать few-shot обучение для руководства ответами модели, предоставляя несколько примеров. Кроме того, мы можем явно указать модели форматировать ответы определенным образом.\n",
        "\n",
        "В следующих разделах мы покажем вам, как сделать аналогичный базовый RAG с некоторыми популярными RAG-фреймворками.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain langchain-mistralai langchain_community mistralai==0.4.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# Загрузка данных\n",
        "loader = TextLoader(\"essay.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Разделение текста на чанки\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "\n",
        "# Определение модели эмбеддингов\n",
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n",
        "\n",
        "# Создание векторного хранилища\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Определение интерфейса ретривера\n",
        "retriever = vector.as_retriever()\n",
        "\n",
        "# Определение LLM\n",
        "model = ChatMistralAI(mistral_api_key=api_key)\n",
        "\n",
        "# Определение шаблона промпта\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Ответьте на следующий вопрос только на основе предоставленного контекста:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Вопрос: {input}\"\"\")\n",
        "\n",
        "# Создание цепочки поиска для ответов на вопросы\n",
        "document_chain = create_stuff_documents_chain(model, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "response = retrieval_chain.invoke({\"input\": \"Какие две основные вещи автор делал до колледжа?\"})\n",
        "print(response[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LlamaIndex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index==0.10.55 llama-index-llms-mistralai==0.1.18 llama-index-embeddings-mistralai mistralai==0.4.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "\n",
        "# Загрузка данных\n",
        "reader = SimpleDirectoryReader(input_files=[\"essay.txt\"])\n",
        "documents = reader.load_data()\n",
        "\n",
        "# Определение LLM и модели эмбеддингов\n",
        "Settings.llm = MistralAI(model=\"mistral-medium\", api_key=api_key)\n",
        "Settings.embed_model = MistralAIEmbedding(model_name='mistral-embed', api_key=api_key)\n",
        "\n",
        "# Создание индекса векторного хранилища\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Создание движка запросов\n",
        "query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "response = query_engine.query(\n",
        "    \"Какие две основные вещи автор делал до колледжа?\"\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Haystack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install mistral-haystack==0.0.1 mistralai==0.4.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.utils.auth import Secret\n",
        "\n",
        "from haystack.components.builders import DynamicChatPromptBuilder\n",
        "from haystack.components.converters import TextFileToDocument\n",
        "from haystack.components.preprocessors import DocumentSplitter\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack_integrations.components.embedders.mistral import MistralDocumentEmbedder, MistralTextEmbedder\n",
        "from haystack_integrations.components.generators.mistral import MistralChatGenerator\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "docs = TextFileToDocument().run(sources=[\"essay.txt\"])\n",
        "split_docs = DocumentSplitter(split_by=\"passage\", split_length=2).run(documents=docs[\"documents\"])\n",
        "embeddings = MistralDocumentEmbedder(api_key=Secret.from_token(api_key)).run(documents=split_docs[\"documents\"])\n",
        "DocumentWriter(document_store=document_store).run(documents=embeddings[\"documents\"])\n",
        "\n",
        "text_embedder = MistralTextEmbedder(api_key=Secret.from_token(api_key))\n",
        "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
        "prompt_builder = DynamicChatPromptBuilder(runtime_variables=[\"documents\"])\n",
        "llm = MistralChatGenerator(api_key=Secret.from_token(api_key), model='mistral-small')\n",
        "\n",
        "chat_template = \"\"\"Ответьте на следующий вопрос на основе содержимого документов.\\n\n",
        "                Вопрос: {{query}}\\n\n",
        "                Документы:\n",
        "                {% for document in documents %}\n",
        "                    {{document.content}}\n",
        "                {% endfor%}\n",
        "                \"\"\"\n",
        "messages = [ChatMessage.from_user(chat_template)]\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "rag_pipeline.add_component(\"retriever\", retriever)\n",
        "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "rag_pipeline.add_component(\"llm\", llm)\n",
        "\n",
        "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
        "\n",
        "question = \"Какие две основные вещи автор делал до колледжа?\"\n",
        "\n",
        "result = rag_pipeline.run(\n",
        "    {\n",
        "        \"text_embedder\": {\"text\": question},\n",
        "        \"prompt_builder\": {\"template_variables\": {\"query\": question}, \"prompt_source\": messages},\n",
        "        \"llm\": {\"generation_kwargs\": {\"max_tokens\": 225}},\n",
        "    }\n",
        ")\n",
        "\n",
        "print(result[\"llm\"][\"replies\"][0].content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Заключение\n",
        "\n",
        "Это руководство продемонстрировало, как создать базовую систему RAG четырьмя различными способами:\n",
        "1. **С нуля** используя Faiss для векторной базы данных\n",
        "2. **С LangChain** для более структурированного подхода с цепочками\n",
        "3. **С LlamaIndex** для простого подхода с индексами\n",
        "4. **С Haystack** для пайплайн-ориентированного подхода\n",
        "\n",
        "Каждый подход имеет свои преимущества и может быть выбран в зависимости от ваших конкретных потребностей и предпочтений в архитектуре приложения.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
